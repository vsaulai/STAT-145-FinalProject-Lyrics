---
title: "DataVisualizationCodeSV"
author: "Saulai Vue"
date: "2025-05-15"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(tidyverse)
library(rvest) #web scraping package and working with html
library(tidytext) #converts text into tidy format, and text mining
library(httr)    #sends http requests and working with web API
library(stringr) #working with strings into tidyverse and cleaning text strings
library(readr)
library(ggplot2)
library(gganimate)
library(transformr)
library(scales)
library(wordcloud)
library(RColorBrewer)
library(purrr)
library(litedown) # dependency of ggwordcloud
library(markdown) # dependency of ggwordcloud
library(jpeg) # dependency of ggwordcloud
library(gridtext) # dependency of ggwordcloud
library(ggwordcloud)
```



## Import Main Data Frame

```{r}
lyrics_final_df_url <- "https://raw.githubusercontent.com/vsaulai/STAT-345-FinalProject-Lyrics/refs/heads/main/final_lyrics_df.csv"
lyrics_df <- read.csv(lyrics_final_df_url)
View(lyrics_df)
```

## Function to Remove all NA values in Lyrics

```{r}
delete_na_rows <- function(df, column_name) {
  # Check if the column exists
  if (!column_name %in% names(df)) {
    stop("The specified column does not exist in the data frame.")
  }
  
  # Remove rows where the specified column is NA or blank
  df_clean <- df[!is.na(df[[column_name]]) & df[[column_name]] != "", ]
  
  return(df_clean)
}
```

## Run Previous Function

```{r}
lyrics_df_remove_na <- delete_na_rows(lyrics_df, "lyrics")

```

## Remove Unnecessary Columns
- Eventually, adding the row sums of total words estimated by udpipe model and finding the proportion of parts of speech.
- Therefore, removing all non words that showed up in the data count to get the total.

```{r}
lyrics_df_count <- lyrics_df_remove_na[ , !(names(lyrics_df_remove_na) %in% c("url", "other", "symbol", "numeral"))]

```


## Word Count from Udpipe Language Model

```{r}
lyrics_df_count <- lyrics_df_count %>%
  mutate(row_total = rowSums(.[ , 9:23], na.rm = TRUE))
lyrics_df_count <- lyrics_df_count %>%
  rename(pos_total = row_total)
```


## Average Word Count List (Floating Point)

```{r}
yearly_averages <- lyrics_df_count %>%
  group_by(Year) %>%
  summarize(
    avg_title_count = mean(title_repetition_count, na.rm = TRUE),
    avg_word_count = mean(word_count, na.rm = TRUE),
    avg_adjective = mean(adjective, na.rm = TRUE),
    avg_adposition = mean(adposition, na.rm = TRUE),
    avg_adverb = mean(adverb, na.rm = TRUE),
    avg_auxiliary_verb = mean(AUX, na.rm = TRUE),
    avg_coordinating_conj = mean(coordinating_conj, na.rm = TRUE),
    avg_determiner = mean(determiner, na.rm = TRUE),
    avg_interjection = mean(interjection, na.rm = TRUE),
    avg_noun = mean(noun, na.rm = TRUE),
    avg_particle = mean(particle, na.rm = TRUE),
    avg_pronoun = mean(pronoun, na.rm = TRUE),
    avg_proper_noun = mean(proper_noun, na.rm = TRUE),
    avg_verb = mean(verb, na.rm = TRUE),
    avg_punctuation = mean(punctuation, na.rm = TRUE),
    avg_subordinating_conj = mean(subordinating_conj, na.rm = TRUE),
    avg_NA = mean(`NA.`, na.rm = TRUE),
    avg_pos_total = mean(pos_total, na.rm = TRUE),
    .groups = 'drop'
  )

```


## Average Word Count List (Rounded up to Two Decimal Points)

```{r}
yearly_averages_rounded <- lyrics_df_count %>%
  group_by(Year) %>%
  summarize(
    avg_title_count = round(mean(title_repetition_count, na.rm = TRUE), 2),
    avg_word_count = round(mean(word_count, na.rm = TRUE), 2),
    avg_adjective = round(mean(adjective, na.rm = TRUE), 2),
    avg_adposition = round(mean(adposition, na.rm = TRUE), 2),
    avg_adverb = round(mean(adverb, na.rm = TRUE), 2),
    avg_auxiliary_verb = round(mean(AUX, na.rm = TRUE), 2),
    avg_coordinating_conj = round(mean(coordinating_conj, na.rm = TRUE), 2),
    avg_determiner = round(mean(determiner, na.rm = TRUE), 2),
    avg_interjection = round(mean(interjection, na.rm = TRUE), 2),
    avg_noun = round(mean(noun, na.rm = TRUE), 2),
    avg_particle = round(mean(particle, na.rm = TRUE), 2),
    avg_pronoun = round(mean(pronoun, na.rm = TRUE), 2),
    avg_proper_noun = round(mean(proper_noun, na.rm = TRUE), 2),
    avg_verb = round(mean(verb, na.rm = TRUE), 2),
    avg_punctuation = round(mean(punctuation, na.rm = TRUE), 2),
    avg_subordinating_conj = round(mean(subordinating_conj, na.rm = TRUE), 2),
    avg_NA = round(mean(`NA.`, na.rm = TRUE), 2),
    avg_pos_total = round(mean(pos_total, na.rm = TRUE), 2),
    .groups = 'drop'
  )
```



```{r}
combined_averages <- list()

# Loop over the names (e.g., years) in the lists
for (Year in names(yearly_averages)) {
  combined_averages[[Year]] <- list(
    unrounded = yearly_averages[[Year]],
    rounded = yearly_averages_rounded[[Year]]
  )
}

```


## Splitting into Periods by Decades into A List

1958-1969, 1970-1979, 1980-1989, 1990-1999, 2000-2009, 2010-2019, 2020-2024.

```{r}
# Create a list of named intervals
year_intervals <- list(
  "1958_1969" = c(1958, 1969),
  "1970_1979" = c(1970, 1979),
  "1980_1989" = c(1980, 1989),
  "1990_1999" = c(1990, 1999),
  "2000_2009" = c(2000, 2009),
  "2010_2019" = c(2010, 2019),
  "2020_2024" = c(2020, 2024)
)

# Create a list to hold the split data frames
split_years <- list()

# For Loop to Split Years into Desired intervals
for (name in names(year_intervals)) {
  range <- year_intervals[[name]]
  start <- range[1]
  end <- range[2]
  
  # Filter the data for this range
  df_range <- subset(lyrics_df_count, Year >= start & Year <= end)
  
  # Save to environment and list
  assign(paste0("lyrics_", name), df_range, envir = .GlobalEnv)
  split_years[[paste0("lyrics_", name)]] <- df_range
}

```

## Splitting POS types into categories


```{r}
# 1958â€“1969
ratio_1958_1969 <- lyrics_1958_1969 %>% # Taking the proportion of each song's POS type to total POS word for each year
  mutate(
    nouns_group = noun / pos_total,
    proper_nouns_group = proper_noun / pos_total, # Proper Nouns are significant because it's naming a direct person, place, or thing. Which may hint towards a decade's cultural reference.
    verbs_group = (verb + AUX )/ pos_total, # Verbs + Auxiliary Verbs were my preference of POS category, Actions are significant POS word types.
    adjectives_group = adjective / pos_total, # Adjectives are descriptive POS and are significant to stand alone 
    adverbs_group = adverb / pos_total + particle / pos_total, # Adverbs are describing actions and may have an argument to be in verbs but I chose to leave it separated. Also contains particles which are of the same function like adverbs i.e. ("up", "on", "out", I ran "into" my professor)
    pronouns_group = pronoun / pos_total, # Pronouns operate like proper nouns, Hinting at specifically someone i.e.(Him, Her, They, It, He, She, etc.) could be referencing an Ex boyfriend or girlfriend. So, I thought this to be significant to stand out.
    interjections_group = interjection / pos_total, # Interjections are emotional words like "Hurray!", "Hey", "Ow", they can oftentimes be related to slang which I though significant
    punctuation_group = punctuation / pos_total, # Punctuation may not seem relevant but it is sometimes indicating slang or 'double' words like can't, shouldn't, Ain't, etc. Slang words are pretty significant.
    # Function Word Group is everything else: determiners ("a", "an", "the"); conjunctions("for", "and", "but", "whereas", "unless"); 
    #  `NA.` is other words that the language model did not comprehend what POS type the word was.
    function_word_group = (determiner + coordinating_conj + subordinating_conj + adposition + `NA.`) / pos_total,
  ) %>%
  summarise( # Combining each song to each decade category and repeat for each decade interval
    decade = "1958_1969",
    nouns = mean(nouns_group, na.rm = TRUE),
    proper_nouns = mean(proper_nouns_group, na.rm = TRUE),
    verbs = mean(verbs_group, na.rm = TRUE),
    adjectives = mean(adjectives_group, na.rm = TRUE),
    adverbs = mean(adverbs_group, na.rm = TRUE),
    pronouns = mean(pronouns_group, na.rm = TRUE),
    interjections = mean(interjections_group, na.rm = TRUE),
    punctuation = mean(punctuation_group, na.rm = TRUE),
    function_words = mean(function_word_group, na.rm = TRUE)
  ) 
# 1970-1979
ratio_1970_1979 <- lyrics_1970_1979 %>%
  mutate( 
    nouns_group = noun / pos_total,
    proper_nouns_group = proper_noun / pos_total,
    verbs_group = (verb + AUX )/ pos_total,
    adjectives_group = adjective / pos_total,
    adverbs_group = adverb / pos_total + particle / pos_total,
    pronouns_group = pronoun / pos_total,
    interjections_group = interjection / pos_total,
    punctuation_group = punctuation / pos_total,
    function_word_group = (determiner + coordinating_conj + subordinating_conj + adposition + `NA.`) / pos_total,
  ) %>%
  summarise(
    decade = "1970_1979",
    nouns = mean(nouns_group, na.rm = TRUE),
    proper_nouns = mean(proper_nouns_group, na.rm = TRUE),
    verbs = mean(verbs_group, na.rm = TRUE),
    adjectives = mean(adjectives_group, na.rm = TRUE),
    adverbs = mean(adverbs_group, na.rm = TRUE),
    pronouns = mean(pronouns_group, na.rm = TRUE),
    interjections = mean(interjections_group, na.rm = TRUE),
    punctuation = mean(punctuation_group, na.rm = TRUE),
    function_words = mean(function_word_group, na.rm = TRUE)
  )
# 1980-1989
ratio_1980_1989 <- lyrics_1980_1989 %>%
  mutate(
    nouns_group = noun / pos_total,
    proper_nouns_group = proper_noun / pos_total,
    verbs_group = (verb + AUX )/ pos_total,
    adjectives_group = adjective / pos_total,
    adverbs_group = adverb / pos_total + particle / pos_total,
    pronouns_group = pronoun / pos_total,
    interjections_group = interjection / pos_total,
    punctuation_group = punctuation / pos_total,
    function_word_group = (determiner + coordinating_conj + subordinating_conj + adposition + `NA.`) / pos_total,
  ) %>%
  summarise(
    decade = "1980_1989",
    nouns = mean(nouns_group, na.rm = TRUE),
    proper_nouns = mean(proper_nouns_group, na.rm = TRUE),
    verbs = mean(verbs_group, na.rm = TRUE),
    adjectives = mean(adjectives_group, na.rm = TRUE),
    adverbs = mean(adverbs_group, na.rm = TRUE),
    pronouns = mean(pronouns_group, na.rm = TRUE),
    interjections = mean(interjections_group, na.rm = TRUE),
    punctuation = mean(punctuation_group, na.rm = TRUE),
    function_words = mean(function_word_group, na.rm = TRUE)
  )
# 1990-1999
ratio_1990_1999 <- lyrics_1990_1999 %>%
  mutate(
    nouns_group = noun / pos_total,
    proper_nouns_group = proper_noun / pos_total,
    verbs_group = (verb + AUX )/ pos_total,
    adjectives_group = adjective / pos_total,
    adverbs_group = adverb / pos_total + particle / pos_total,
    pronouns_group = pronoun / pos_total,
    interjections_group = interjection / pos_total,
    punctuation_group = punctuation / pos_total,
    function_word_group = (determiner + coordinating_conj + subordinating_conj + adposition + `NA.`) / pos_total,
  ) %>%
  summarise(
    decade = "1990_1999",
    nouns = mean(nouns_group, na.rm = TRUE),
    proper_nouns = mean(proper_nouns_group, na.rm = TRUE),
    verbs = mean(verbs_group, na.rm = TRUE),
    adjectives = mean(adjectives_group, na.rm = TRUE),
    adverbs = mean(adverbs_group, na.rm = TRUE),
    pronouns = mean(pronouns_group, na.rm = TRUE),
    interjections = mean(interjections_group, na.rm = TRUE),
    punctuation = mean(punctuation_group, na.rm = TRUE),
    function_words = mean(function_word_group, na.rm = TRUE)
  )
# 2000â€“2009
ratio_2000_2009 <- lyrics_2000_2009 %>%
  mutate(
    nouns_group = noun / pos_total,
    proper_nouns_group = proper_noun / pos_total,
    verbs_group = (verb + AUX )/ pos_total,
    adjectives_group = adjective / pos_total,
    adverbs_group = adverb / pos_total + particle / pos_total,
    pronouns_group = pronoun / pos_total,
    interjections_group = interjection / pos_total,
    punctuation_group = punctuation / pos_total,
    function_word_group = (determiner + coordinating_conj + subordinating_conj + adposition + `NA.`) / pos_total,
  ) %>%
  summarise(
    decade = "2000_2009",
    nouns = mean(nouns_group, na.rm = TRUE),
    proper_nouns = mean(proper_nouns_group, na.rm = TRUE),
    verbs = mean(verbs_group, na.rm = TRUE),
    adjectives = mean(adjectives_group, na.rm = TRUE),
    adverbs = mean(adverbs_group, na.rm = TRUE),
    pronouns = mean(pronouns_group, na.rm = TRUE),
    interjections = mean(interjections_group, na.rm = TRUE),
    punctuation = mean(punctuation_group, na.rm = TRUE),
    function_words = mean(function_word_group, na.rm = TRUE)
  )
# 2010â€“2019
ratio_2010_2019 <- lyrics_2010_2019 %>%
  mutate(
    nouns_group = noun / pos_total,
    proper_nouns_group = proper_noun / pos_total,
    verbs_group = (verb + AUX )/ pos_total,
    adjectives_group = adjective / pos_total,
    adverbs_group = adverb / pos_total + particle / pos_total,
    pronouns_group = pronoun / pos_total,
    interjections_group = interjection / pos_total,
    punctuation_group = punctuation / pos_total,
    function_word_group = (determiner + coordinating_conj + subordinating_conj + adposition + `NA.`) / pos_total,
  ) %>%
  summarise(
    decade = "2010_2019",
    nouns = mean(nouns_group, na.rm = TRUE),
    proper_nouns = mean(proper_nouns_group, na.rm = TRUE),
    verbs = mean(verbs_group, na.rm = TRUE),
    adjectives = mean(adjectives_group, na.rm = TRUE),
    adverbs = mean(adverbs_group, na.rm = TRUE),
    pronouns = mean(pronouns_group, na.rm = TRUE),
    interjections = mean(interjections_group, na.rm = TRUE),
    punctuation = mean(punctuation_group, na.rm = TRUE),
    function_words = mean(function_word_group, na.rm = TRUE)
  )
# 2020â€“2024
ratio_2020_2024 <- lyrics_2020_2024 %>%
  mutate(
    nouns_group = noun / pos_total,
    proper_nouns_group = proper_noun / pos_total,
    verbs_group = (verb + AUX )/ pos_total,
    adjectives_group = adjective / pos_total,
    adverbs_group = adverb / pos_total + particle / pos_total,
    pronouns_group = pronoun / pos_total,
    interjections_group = interjection / pos_total,
    punctuation_group = punctuation / pos_total,
    function_word_group = (determiner + coordinating_conj + subordinating_conj + adposition + `NA.`) / pos_total,
  ) %>%
  summarise(
    decade = "2020_2024",
    nouns = mean(nouns_group, na.rm = TRUE),
    proper_nouns = mean(proper_nouns_group, na.rm = TRUE),
    verbs = mean(verbs_group, na.rm = TRUE),
    adjectives = mean(adjectives_group, na.rm = TRUE),
    adverbs = mean(adverbs_group, na.rm = TRUE),
    pronouns = mean(pronouns_group, na.rm = TRUE),
    interjections = mean(interjections_group, na.rm = TRUE),
    punctuation = mean(punctuation_group, na.rm = TRUE),
    function_words = mean(function_word_group, na.rm = TRUE)
  )
full_proportion_summary <- bind_rows(
  ratio_1958_1969,
  ratio_1970_1979,
  ratio_1980_1989,
  ratio_1990_1999,
  ratio_2000_2009,
  ratio_2010_2019,
  ratio_2020_2024,
)
```


# Converting to Long Format

```{r}
long_proportion_summary  <- full_proportion_summary  %>%
  pivot_longer(
    cols = -decade, 
    names_to = "pos", 
    values_to = "proportion"
    )
```

# Computing Z-Scores within each POS Group

- scale() standardizes the proportion within each POS category â€” so now we can see how each decade deviates from the average for that POS.

```{r}
z_score_prop_summary <- long_proportion_summary %>%
  group_by(pos) %>%
  mutate(z_score = scale(proportion)) %>%
  ungroup()
```


## Heat Map for Z-Score

```{r}
ggplot(z_score_prop_summary, aes(x = decade, y = pos, fill = z_score)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red", midpoint = 0,
    name = "Z-Score"
  ) +
  theme_minimal(base_size = 14) +
  labs(
    title = "Z-Score Normalized Part-of-Speech Use by Decade",
    x = "Decade",
    y = "Part of Speech"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

Z-score > 0 :
The proportion of that POS in this decade is above average compared to all decades.
The higher the value, the more unusually high that POS usage was in that decade.

Z-score < 0 :
The proportion of that POS in this decade is below average.

Z-score = 0 :
The proportion of POS use is very close across all decades.

```{r}
head(long_proportion_summary)

```


## Animated Stacked Area Plot Code

```{r}
full_proportion_summary <- full_proportion_summary %>%
  mutate(decade = str_replace_all(decade, "-", "_")) %>%
  mutate(decade = factor(decade, levels = c(
    "1958_1969", "1970_1979", "1980_1989", "1990_1999",
    "2000_2009", "2010_2019", "2020_2024"
  )))

```

```{r}
long_proportion_summary <- long_proportion_summary %>%
  mutate(decade = factor(decade, levels = c(
    "1958_1969", "1970_1979", "1980_1989", "1990_1999",
    "2000_2009", "2010_2019", "2020_2024"
  )))

```


```{r}
ggplot(long_proportion_summary, aes(x = pos, y = proportion, fill = pos)) +
  geom_col() +
  scale_y_continuous(labels = percent_format()) +
  labs(
    title = "POS Word Type Proportions in Top Songs: {closest_state}",
    x = NULL,  # No label
    y = "Proportion of Total POS Words",
    fill = "POS Type"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.ticks.x = element_blank()     # Remove x-axis ticks
  ) +
  transition_states(decade, transition_length = 3, state_length = 1) +
  ease_aes('cubic-in-out')

```


```{r}
anim <- last_plot()
animate(anim, duration = 10, fps = 15, width = 800, height = 600, renderer = gifski_renderer("pos_animation.gif"))
```


## Average Amount of Words Count per Year(Top 20 Billboard Songs)

- Takes every Top 17-20 Billboard Songs Per Year,

```{r}
wordcount_by_year <- bind_rows(yearly_averages)

ggplot(wordcount_by_year, aes(x = as.numeric(Year), y = avg_word_count)) +
  geom_line(color = "steelblue", size = 1) +
  geom_point(color = "red", size = 1.5) +
  geom_smooth(method = "lm", color = "darkgreen", linetype = "dashed", se = TRUE) +
  labs(
    title = "Average Word Count per Year (Top 20 Billboard Songs)",
    x = "Year (1958-2024)",
    y = "Average Word Count"
  ) +
  theme_minimal()

```

```{r}
noun_by_year <- bind_rows(yearly_averages)

ggplot(wordcount_by_year, aes(x = as.numeric(Year), y = avg_noun + avg_proper_noun)) +
  geom_line(color = "steelblue", size = 1) +
  geom_point(color = "darkred", size = 1.5) +
  geom_smooth(method = "lm", color = "black", linetype = "dashed", se = TRUE) +
  labs(
    title = "Average Nouns in Lyrics per Year (Top 20 Billboard Songs)",
    x = "Year (1958-2024)",
    y = "Average Noun Count"
  ) +
  theme_minimal()

```


```{r}
# List of unwanted words to exclude
unwanted_words <- c("the", "and", "to", "a", "it", "it's", "an", "is", "on", "so", "at", "too", "off", 
                    "but", "that", "there", "their", "was", "by", "my", "your", "are", "am", "i'm", 
                    "this", "has", "had", "in", "of", "i'd", "i've", "oh", "these", "into", "as", "no", 
                    "ya", "only", "ah", "be", "do", "you'll", "who", "what", "when", "where")

count_words_from_df <- function(df, column_name, exclude = NULL) {
  if (!requireNamespace("stringr", quietly = TRUE)) install.packages("stringr")
  library(stringr)

  if (!column_name %in% names(df)) {
    stop("The specified column does not exist in the data frame.")
  }

  lyrics_text <- paste(df[[column_name]], collapse = " ")
  lyrics_text <- tolower(lyrics_text)
  lyrics_text <- str_replace_all(lyrics_text, "[0-9]", " ")
  lyrics_text <- str_squish(lyrics_text)

  words <- unlist(str_split(lyrics_text, "\\s+"))

  if (!is.null(exclude)) {
    # Remove punctuation from both sides before comparison
    words_clean <- str_replace_all(words, "[[:punct:]]", "")
    exclude_clean <- str_replace_all(exclude, "[[:punct:]]", "")
    words <- words[!words_clean %in% exclude_clean]
  }

  word_counts <- table(words)
  word_df <- as.data.frame(word_counts, stringsAsFactors = FALSE)
  colnames(word_df) <- c("word", "count")
  word_df <- word_df[order(-word_df$count), ]

  return(word_df)
}

# Use the function with unwanted words passed in
counting_lyrics <- count_words_from_df(lyrics_df_count, "lyrics", exclude = unwanted_words)

```

# Wordcloud of the Entire Data Set
(Removed most function words e.g. "a", "an", "it", etc.)
```{r}
wordcloud(
  words = counting_lyrics$word,
  freq = counting_lyrics$count,
  min.freq = 100,                    # Only include words that appear 2+ times
  max.words = 200,                 # Limit number of words in the cloud
  random.order = FALSE,           # Plot most frequent words in center
  colors = brewer.pal(7, "Dark2") # Use a nice color palette
)

```


