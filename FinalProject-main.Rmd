---
title: "FinalProjectLyrics"
author: "Saulai Vue, Jake Christenson, Kaleb Laramore"
date: "2025-04-11"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(tidyverse)
library(rvest) #web scraping package and working with html
library(tidytext) #converts text into tidy format, and text mining
library(httr)    #sends http requests and working with web API
library(stringr) #working with strings into tidyverse and cleaning text strings
library(readr)
library(ggplot2)
library(gganimate)
library(transformr)
library(scales)
library(wordcloud)
library(RColorBrewer)
library(purrr)
```


```{r, include=FALSE}
# Main Data Frame
lyrics_final_df_url <- "https://raw.githubusercontent.com/vsaulai/STAT-345-FinalProject-Lyrics/refs/heads/main/final_lyrics_df.csv"
lyrics_df <- read.csv(lyrics_final_df_url)
```


# Part 1: 
Your first task is to create a list of top songs, dating back to 1958 (when Billboard introduced it's Hot 100 yearly chart). You may want to start with just the yearly top song, but your work should be general enough to account for multiple songs per year. You may narrow your search to a particular genre if you like. You may use any website that provides this information, though you may try to find one that makes part 2 as simple as possible.

- Initially, we scraped from 1958-2024 for all Top 20 Billboard Songs from wikipedia. This function generated our list of songs and artist in our data frame.

-  With this, we wrote a function to be used to webscrape for artist and song titles from wikipedia.

```{r, eval=FALSE, include=FALSE}
#Scrape the 1958 Top 20 Songs
scrape_1958_top_20 <- function() {
  # URL for 1958 data
  url <- "https://en.wikipedia.org/wiki/Billboard_year-end_top_50_singles_of_1958"
  # Read the page
  page <- read_html(url)
  # Extract the tables and select the first one
  tables <- html_nodes(page, "table")
  table_of_interest <- html_table(tables[[1]], fill = TRUE)
  # Extract the top 20 rows and add "Year" and "Rank" columns
  top_20_songs <- tibble(table_of_interest[1:20, ], Year = 1958, Rank = 1:20)
  return(top_20_songs)
}

#Scrape 1959–2024 Top 20 Songs
scrape_top_20_songs <- function(year) {
  # Construct the URL dynamically
  url <- paste0("https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_", year)
  # Try to read the page with error handling
  page <- tryCatch(read_html(url), error = function(e) NULL)
  on.exit(rm(page), add = TRUE)  # Clean up memory
  gc()
  # Check if the page is accessible
  if (is.null(page)) {
    message(paste("Could not retrieve data for year:", year))
    write(paste("Failed to retrieve:", year), file = "failed_urls.txt", append = TRUE)
    return(NULL)
  }
  # Extract tables and select the first one
  tables <- html_nodes(page, "table")
  if (length(tables) < 1) {
    message(paste("No table found for year:", year))
    write(paste("No table found:", year), file = "failed_urls.txt", append = TRUE)
    return(NULL)
  }
  table_of_interest <- html_table(tables[[1]], fill = TRUE)
  # Extract the top 20 rows and add "Year" and "Rank" columns
  top_20_songs <- tibble(table_of_interest[1:20, ], Year = year, Rank = 1:20)
  return(top_20_songs)
}

#Scrape the 1958 data
message("Scraping data for 1958...")
# data_1958 <- scrape_1958_top_20()

#Scrape the 1959–2024 data
years <- 1959:2024  # Define the range of years
all_years_data <- list()  # Initialize list to store yearly data

for (year in years) {
  message(paste("Scraping data for year:", year))
  year_data <- scrape_top_20_songs(year)
  if (!is.null(year_data)) {
    all_years_data[[as.character(year)]] <- year_data
  }
}
# Combine data from all years (1959–2024)
# data_1959_2024 <- bind_rows(all_years_data)
#Combine the 1958 and 1959–2024 datasets
# combined_data <- bind_rows(data_1958, data_1959_2024)
#Retain Only Relevant Columns
# cleaned_data <- combined_data %>%
#  select(Title, `Artist(s)`, Year, Rank)
# Save the Cleaned Data to a CSV File
# write.csv(cleaned_data, "Top_20_Songs_1958_2024.csv", row.names = FALSE)
```

### Extra File Generating Artist and Song Names

Attached are extended files from other markdown files that helped in webscraping

- From Github file code (https://raw.githubusercontent.com/vsaulai/STAT-345-FinalProject-Lyrics/refs/heads/main/webscraping/JakeChristiansenCode.Rmd)

- Note: Some markdown files were created by different group members and worked at different times while working on the group project. Files may not be integrated seemlessly together at the moment.
        
```{r, eval=FALSE, include=FALSE}
url <- "https://raw.githubusercontent.com/vsaulai/STAT-345-FinalProject-Lyrics/refs/heads/main/webscraping/JakeChristiansenCode.Rmd"
download.file(url, destfile = "wiki_scrape_songlist.Rmd", mode = "wb")
file.edit("wiki_scrape_songlist.Rmd")
```

# Part 2:
For the top songs in part 1, gather some basic information: artist, title, year, genre (if appropriate), length, and other variables you think might be informative (sales figures, etc.).
While webscraping we wanted genres to be incorporated with our data frame. However, finding a site that can freely give us the genre type of a song was challenging so we abandoned this idea. Next

- We initially wanted a genre incorporated in our data set but it was beyond our ability. While attempting to scrape for genres, it did not work and had problems in the CSS selector or website host.
- Therefore, we just settled on: artist, song title, year, and rank of song. Eventually, we added other information like: common phrases repeated, number of times the song title is repeated in lyrics, and word count. 
- Next, I imported the 'udpipe' language model and wrote a function to count the word types for each song lyrics and got these amounts in our data frame, and will eventually add word type counts to the data set.


# Part 3: 
Find a lyric hosting service (such as www.azlyrics.com or www.songlyrics.com, though these aren't the only options) that provides full lyrics to songs. Ideally, the URLs for these songs follow a reproducible pattern. Write a function that can automatically capture these song lyrics for your top songs from part 1, and then gather the lyrics. Do your best to keep this function general, but you may need to write code for specific instances.

- Next, we chose the site host "songlyrics.com" by using the artist and song name we generated previously, we wrote a function to generate a url from the site host. With this, we wrote two main functions: one for generating the URL and one for webscraping.

### URL Generate Function
```{r, eval=FALSE, include=FALSE}
generate_songlyrics_urls <- function(df, artist_col, song_col, base_url_template) {
  # Format for URL changing '-' and lowercase
  format_for_url <- function(x) {
    x <- tolower(x)                 
    x <- gsub("[^a-z0-9 ]", "", x)  # remove special characters
    x <- gsub(" +", "-", x)         # replace spaces with hyphens
    return(x)
  }
  # Extract base URL structure 4th Argument
  base_parts <- unlist(strsplit(base_url_template, "/"))
  base_domain <- paste(base_parts[1:3], collapse = "/")  # "https://www.songlyrics.com"
  
  # Generate URLs
  urls <- mapply(function(artist, song) {
    artist_url <- format_for_url(artist)
    song_url <- format_for_url(song)
    paste0(base_domain, "/", artist_url, "/", song_url, "-lyrics/")
  }, df[[artist_col]], df[[song_col]])
  
  return(urls)
}
```

This generated our URL from our Main Data Frame. Here is an example of the URL we generated.

```{r, echo=FALSE}
url_before_scrape <- lyrics_df[["url"]][1]
cat(url_before_scrape)
```

## Webscraping(cont.)

For the Second Function we took the URL we generated from the previous function and did the webscraping.
  
```{r, eval=FALSE, include=FALSE}
scrape_lyrics_from_urls <- function(df, url_col, lyrics_col_name = "lyrics") {
  get_lyrics <- function(url) {
    tryCatch({ #tryCatch safely handle issues with Scraping returning NA in such cases like a Network Error
      webpage <- read_html(url) #Read HTML from URL
      
      # Extract lyrics from the page
      # CSS Selector
      lyrics_node <- html_node(webpage, "#songLyricsDiv")
      
      # Get text and clean it up
      lyrics <- html_text(lyrics_node)
      lyrics <- trimws(lyrics)
      return(lyrics)
    }, error = function(e) {
      return(NA)  # Return NA on failure
    })
  }
  # Apply the scraping function to each URL
  df[[lyrics_col_name]] <- sapply(df[[url_col]], get_lyrics)

  return(df)
}
```

\newpage

Here is an example of what the scraped lyrics turned out to be.

### Song: "You and I" (1983) by Eddie Rabbit and Crystal Gayle 
```{r, echo=FALSE}
lyrics_sample_eddie_rabbit <- lyrics_df[["lyrics"]][523]
cat(lyrics_sample_eddie_rabbit)
```


```{r, include=FALSE}
lyrics_final_df_url <- "https://raw.githubusercontent.com/vsaulai/STAT-345-FinalProject-Lyrics/refs/heads/main/final_lyrics_df.csv"
lyrics_df <- read.csv(lyrics_final_df_url)
head(lyrics_df[, 1:6], n = 2)
```

### Extra File from Webscraping and Formatting

The two functions were used in these seperate markdown files to help generate our final data set. Download Code is located below and from github.
- From file code (https://raw.githubusercontent.com/vsaulai/STAT-345-FinalProject-Lyrics/refs/heads/main/webscraping/SaulaiVueCode.Rmd) AND (https://raw.githubusercontent.com/vsaulai/STAT-345-FinalProject-Lyrics/refs/heads/main/webscraping/missingSVCode.Rmd)

        
```{r, eval=FALSE, include=FALSE}
# These are seperate markdown files used to webscrape for our main data frame
# Download Code for URL generate, and Webscrape
url <- "https://raw.githubusercontent.com/vsaulai/STAT-345-FinalProject-Lyrics/refs/heads/main/webscraping/SaulaiVueCode.Rmd"
download.file(url, destfile = "scrape_lyrics_files.Rmd", mode = "wb")
file.edit("scrape_lyrics_files.Rmd")

# Download Code for Webscrape, and formatting data frame
url <- "https://raw.githubusercontent.com/vsaulai/STAT-345-FinalProject-Lyrics/refs/heads/main/webscraping/missingSVCode.Rmd"
download.file(url, destfile = "scrape_lyrics_files2.Rmd", mode = "wb")
file.edit("scrape_lyrics_files2.Rmd")
```


# Data Formatting:

- To format the lyric strings to get our main data set, we removed all punctuation(except apostrophe), unnecessary spaces, words within brackets, and section titles like [Verse], [Chorus], [Artist Duet Part], and other random words that may have unintentionally appeared during webscraping.

- Next, we implemented other functions to our lyrics to get the most common words used (phrase_repetition), and amount of times the title (title_repetition_count) was repeated in. And renamed our column titles.


## Udpipe Language Model

- 'udpipe' package is a language model used for tagging, converting words, and defining words by their parts of speech interpreted by the model from their semantic context.
- In our case, we're using this model to estimate the word's POS(Parts of Speech) category, i.e. noun, verb, adjective, etc. from the context of the song lyric.


```{r, eval=FALSE, include=FALSE}
install.packages("udpipe")
library(udpipe)
model <- udpipe_download_model(language = "english") #import english model
ud_model <- udpipe_load_model(model$file_model)
```

## Parts of Speech Counted by Function (our column names)

- These are UPOS(Universal Parts of Speech) tagset categories found in our data set from columns 10 - 27.
- noun, verb, adjective, adposition, auxilary verb, adverb, proper_noun, interjection, numeral, particle, pronoun, subordinating_conj, coordinating_conj, determiner, punctuation, symbol, other,



## Function to Count Word Type Using Udpipe
- Wrote a function that tags words in a string, trims white spaces, annotates and counts the word type in string and enters it back into the data frame.
- This counts the tokenized tags of each word and stores it as an integer count.

```{r, eval=FALSE, include=FALSE}
count_pos_types <- function(df, column_name, ud_model) {
  # Store results in a list
  pos_counts_list <- lapply(df[[column_name]], function(text) {
    if (is.na(text) || trimws(text) == "") return(data.frame())
    # Annotate the text
    annotation <- udpipe_annotate(ud_model, x = text)
    anno_df <- as.data.frame(annotation)
    # Count POS tags
    table(anno_df$upos)
  })
  # Convert to a tidy data frame
pos_summary <- do.call(rbind, lapply(seq_along(pos_counts_list), function(i) {
  pos_counts <- pos_counts_list[[i]]
  
  if (!is.null(pos_counts) && length(pos_counts) > 0) {
    row <- as.data.frame(as.table(pos_counts))
    row$id <- i
    return(row)
  } else {
    return(data.frame(Var1 = NA, Freq = NA, id = i))
  }
}))
  # Reshape to wide format: one row per observation, POS tags as columns+
  pos_wide <- pivot_wider(pos_summary, names_from = Var1, values_from = Freq, values_fill = 0)
  # Join with original data frame (by row index)
  df$id_tmp <- seq_len(nrow(df))
  result <- merge(df, pos_wide, by.x = "id_tmp", by.y = "id", all.x = TRUE)
  result$id_tmp <- NULL
  return(result)
}
```

## Extra File from Data Formatting

The two functions were used in these seperate markdown files to help generate our final data set. 

- From file code (https://raw.githubusercontent.com/vsaulai/STAT-345-FinalProject-Lyrics/refs/heads/main/DataFormattingSV.Rmd)

- Note: Some markdown files were created by different group members working at different on the group project, and are not seemlessly integrated together or required a file that may not be there anymore.

```{r, eval=FALSE, include=FALSE}
r_url <- "https://raw.githubusercontent.com/vsaulai/STAT-345-FinalProject-Lyrics/refs/heads/main/DataFormattingSV.Rmd"
download.file(r_url, destfile = "format_lyrics_files1.Rmd", mode = "wb")
file.edit("format_lyrics_files1.Rmd")
```

## Renaming the Parts of Speech for Clarity

When running the "udpipe" language model with our function it classifies words with the universal POS tags and are indicated below
### Parts of Speech (POS) Tags:

- noun = NOUN  e.g. (office, car, table, freedom, happiness)
- verb = VERB  e.g. (run, jump, sleep, exist, can, was, has)
- adjective  = ADJ  e.g. (big, small, red, nice, American, his, their, smallest)
- adposition = ADP  e.g. (in, on, at, by)
- adverb  = ADV  e.g. (literally, elegantly, quickly, always, here, very)
- auxiliary_verb = AUX  e.g. (might, had, may, shall, ought to, are, is)
- proper_noun = PROPN  e.g. (Sarah, Titanic, Facebook, Microsoft, Rocky Rococo)
- interjection = INTJ  e.g. (Phew, Ahem, Ahoy, Huh, Eww, Hurray, Oh)
- numeral = NUM  e.g. (one, two, three, 1, 2, 3)
- particle = PART  e.g. (up, out, on, to, off, in) i.e. (to know, cut in, jump off)
- pronoun = PRON  e.g. (us, she, he, each, we, I, Them)
- subordinating_conj = SCONJ  e.g. (after, because, even though, once, that, if, unless)
- coordinating_conj = CCONJ  e.g. (for, and, nor, but, or, yet, so)
- determiner = DET  e.g. (a, an, the, my, your, this, that, some)
- punctuation = PUNCT  e.g. (most punctuation, ain't, findin', shan't,)
- symbol = SYM e.g. (Yin and Yang, Peace Sign)
- other = X  (unknowns)

## Final Main Data Frame Created from Webscraping
  - Here is an example of what the udpipe language model tagged from our lyrical text for the first two songs.

```{r, echo=FALSE}
head(lyrics_df[, 9:22], n = 2)
```

### Removing NA values from Songs and Artists
  - Some songs failed from webscrapping, either the website host did not have any lyrics or the url caused problems.
  - Lost approximately +100 potential songs by their rankings.

```{r, echo=FALSE, include=FALSE}
delete_na_rows <- function(df, column_name) {
  # Check if the column exists
  if (!column_name %in% names(df)) {
    stop("The specified column does not exist in the data frame.")
  }
  
  # Remove rows where the specified column is NA or blank
  df_clean <- df[!is.na(df[[column_name]]) & df[[column_name]] != "", ]
  
  return(df_clean)
}
```

```{r, echo=FALSE, include=FALSE}
lyrics_df_remove_na <- delete_na_rows(lyrics_df, "lyrics")
```

### Removing Unnecessary Columns
- Eventually, adding the row sums of total words estimated by udpipe model and finding the proportion of parts of speech.
- Therefore, removing all non words that showed up in the data count to get the total.
- Total Observations: 1190 Songs with Lyrics

```{r, echo=FALSE, include=FALSE}
lyrics_df_count <- lyrics_df_remove_na[ , !(names(lyrics_df_remove_na) %in% c("url", "other", "symbol", "numeral"))]
lyrics_df_count <- lyrics_df_count %>%
  mutate(row_total = rowSums(.[ , 9:23], na.rm = TRUE))
lyrics_df_count <- lyrics_df_count %>%
  rename(pos_total = row_total)
```

```{r, echo=FALSE, include=FALSE}
yearly_averages <- lyrics_df_count %>%
  group_by(Year) %>%
  summarize(
    avg_title_count = mean(title_repetition_count, na.rm = TRUE),
    avg_word_count = mean(word_count, na.rm = TRUE),
    avg_adjective = mean(adjective, na.rm = TRUE),
    avg_adposition = mean(adposition, na.rm = TRUE),
    avg_adverb = mean(adverb, na.rm = TRUE),
    avg_auxiliary_verb = mean(AUX, na.rm = TRUE),
    avg_coordinating_conj = mean(coordinating_conj, na.rm = TRUE),
    avg_determiner = mean(determiner, na.rm = TRUE),
    avg_interjection = mean(interjection, na.rm = TRUE),
    avg_noun = mean(noun, na.rm = TRUE),
    avg_particle = mean(particle, na.rm = TRUE),
    avg_pronoun = mean(pronoun, na.rm = TRUE),
    avg_proper_noun = mean(proper_noun, na.rm = TRUE),
    avg_verb = mean(verb, na.rm = TRUE),
    avg_punctuation = mean(punctuation, na.rm = TRUE),
    avg_subordinating_conj = mean(subordinating_conj, na.rm = TRUE),
    avg_NA = mean(`NA.`, na.rm = TRUE),
    avg_pos_total = mean(pos_total, na.rm = TRUE),
    .groups = 'drop'
  )

```

## Average Word Count List (Rounded up to Two Decimal Points)
- Took the average of each Song and grouped them together with their respective year's average

```{r, echo=FALSE}
yearly_averages_rounded <- lyrics_df_count %>%
  group_by(Year) %>%
  summarize(
    avg_title_count = round(mean(title_repetition_count, na.rm = TRUE), 2),
    avg_word_count = round(mean(word_count, na.rm = TRUE), 2),
    avg_adjective = round(mean(adjective, na.rm = TRUE), 2),
    avg_adposition = round(mean(adposition, na.rm = TRUE), 2),
    avg_adverb = round(mean(adverb, na.rm = TRUE), 2),
    avg_auxiliary_verb = round(mean(AUX, na.rm = TRUE), 2),
    avg_coordinating_conj = round(mean(coordinating_conj, na.rm = TRUE), 2),
    avg_determiner = round(mean(determiner, na.rm = TRUE), 2),
    avg_interjection = round(mean(interjection, na.rm = TRUE), 2),
    avg_noun = round(mean(noun, na.rm = TRUE), 2),
    avg_particle = round(mean(particle, na.rm = TRUE), 2),
    avg_pronoun = round(mean(pronoun, na.rm = TRUE), 2),
    avg_proper_noun = round(mean(proper_noun, na.rm = TRUE), 2),
    avg_verb = round(mean(verb, na.rm = TRUE), 2),
    avg_punctuation = round(mean(punctuation, na.rm = TRUE), 2),
    avg_subordinating_conj = round(mean(subordinating_conj, na.rm = TRUE), 2),
    avg_NA = round(mean(`NA.`, na.rm = TRUE), 2),
    avg_pos_total = round(mean(pos_total, na.rm = TRUE), 2),
    .groups = 'drop'
  )
head(yearly_averages_rounded[, 1:5], n = 4)
```

# Part 4 and 5: 
Create two measures of song repetitiveness. Write a function (or two) to measure song repetitiveness, and apply it to each of the songs from part 1. Suggestions for "repetitiveness" include (but are definitely not limited to): "Do songs repeat the same phrase frequently?" and "Do songs repeat their song title frequently"

```{r, echo=FALSE}
# Title repetition trend visualization
ggplot(yearly_averages, aes(x = Year, y = avg_title_count)) +
  geom_line(color = "steelblue", linewidth = 1) +
  geom_point(color = "blue", size = 2) +
  labs(
    title = "Title Repetition Over Time",
    x = "Year",
    y = "Average Title Repetition Count"
  ) +
  theme_minimal()

```

## Summary of Title Repetition Over Time

Have songs become more repetitive over time? Summarize and visualize your repetitive measures from part 4. 

### Early Years (1958–1980s):
- Song titles appeared less frequently in lyrics.
- Lyrics were more story-driven, focusing on narratives rather than repetition.
- Genres like classic rock, jazz, and folk had minimal reliance on repeated hooks.

### 1990s–2010s:
- Gradual increase in title repetition.
- Pop and hip-hop music adopted catchier hooks, making song titles repeat more often.
- Radio play and streaming services started rewarding songs with memorable choruses.

### 2010s–2020s:
- Significant spike in title repetition.
- Songs now repeat titles multiple times per chorus, making them more recognizable.
- Genres like hip-hop, EDM, and commercial pop use repetition as a marketing tool.

# Part 6: 
(If possible) Extend your work to more songs! Consider more questions, like "Does genre matter?".

## Splitting into Periods by Decades into A List
Next, We split our data into decades.
1958-1969, 1970-1979, 1980-1989, 1990-1999, 2000-2009, 2010-2019, 2020-2024.

```{r, echo=FALSE, include=FALSE}
# Create a list of named intervals
year_intervals <- list(
  "1958_1969" = c(1958, 1969),
  "1970_1979" = c(1970, 1979),
  "1980_1989" = c(1980, 1989),
  "1990_1999" = c(1990, 1999),
  "2000_2009" = c(2000, 2009),
  "2010_2019" = c(2010, 2019),
  "2020_2024" = c(2020, 2024)
)

# Create a list to hold the split data frames
split_years <- list()

# For Loop to Split Years into Desired intervals
for (name in names(year_intervals)) {
  range <- year_intervals[[name]]
  start <- range[1]
  end <- range[2]
  
  # Filter the data for this range
  df_range <- subset(lyrics_df_count, Year >= start & Year <= end)
  
  # Save to environment and list
  assign(paste0("lyrics_", name), df_range, envir = .GlobalEnv)
  split_years[[paste0("lyrics_", name)]] <- df_range
}

```

## Splitting POS types into categories

Taking the proportion of each song's POS type to total POS word for each year

## Categories
- Proper Nouns: are significant because it's naming a direct person, place, or thing. Which may hint towards a decade's cultural reference.
- Verbs + Auxiliary Verbs: were my preference of POS category, Actions are significant POS word types.
- Adjectives: are descriptive POS and are significant to stand alone
- Adverbs: are describing actions and may have an argument to be in verbs but we chose to leave it separated. Also contains particles which are of the same function like adverbs i.e. ("up", "on", "out", I ran "into" my professor)
- Pronouns: hint at specifically someone but not whom i.e.(Him, Her, They, It, He, She, etc.) could be referencing an Ex boyfriend or girlfriend. So, we thought this might be a significant count.
- Interjections: are emotional words like "Hurray!", "Hey", "Ow", they can oftentimes be related to slang which I though significant
- Punctuation: may not seem relevant but it is sometimes indicating slang or 'double' words like can't, shouldn't, Ain't, etc. Slang words are a pretty significant in English Slang.
- Function Word Group: is everything else: Determiners ("a", "an", "the"); Conjunctions both subordinating and coordinating("for", "and", "but", "whereas", "unless"); Adposition ("in", "with", "by") 'NA.' is other words that the language model did not comprehend.

Below is an example of Proportions of POS types

```{r, echo=FALSE, include=FALSE}
# 1958–1969
ratio_1958_1969 <- lyrics_1958_1969 %>% # Taking the proportion of each song's POS type to total POS word for each year
  mutate(
    nouns_group = noun / pos_total,
    proper_nouns_group = proper_noun / pos_total, # Proper Nouns are significant because it's naming a direct person, place, or thing. Which may hint towards a decade's cultural reference.
    verbs_group = (verb + AUX )/ pos_total, # Verbs + Auxiliary Verbs were my preference of POS category, Actions are significant POS word types.
    adjectives_group = adjective / pos_total, # Adjectives are descriptive POS and are significant to stand alone 
    adverbs_group = adverb / pos_total + particle / pos_total, # Adverbs are describing actions and may have an argument to be in verbs but I chose to leave it separated. Also contains particles which are of the same function like adverbs i.e. ("up", "on", "out", I ran "into" my professor)
    pronouns_group = pronoun / pos_total, # Pronouns operate like proper nouns, Hinting at specifically someone i.e.(Him, Her, They, It, He, She, etc.) could be referencing an Ex boyfriend or girlfriend. So, I thought this to be significant to stand out.
    interjections_group = interjection / pos_total, # Interjections are emotional words like "Hurray!", "Hey", "Ow", they can oftentimes be related to slang which I though significant
    punctuation_group = punctuation / pos_total, # Punctuation may not seem relevant but it is sometimes indicating slang or 'double' words like can't, shouldn't, Ain't, etc. Slang words are pretty significant.
    # Function Word Group is everything else: determiners ("a", "an", "the"); conjunctions("for", "and", "but", "whereas", "unless"); 
    #  `NA.` is other words that the language model did not comprehend what POS type the word was.
    function_word_group = (determiner + coordinating_conj + subordinating_conj + adposition + `NA.`) / pos_total,
  ) %>%
  summarise( # Combining each song to each decade category and repeat for each decade interval
    decade = "1958_1969",
    nouns = mean(nouns_group, na.rm = TRUE),
    proper_nouns = mean(proper_nouns_group, na.rm = TRUE),
    verbs = mean(verbs_group, na.rm = TRUE),
    adjectives = mean(adjectives_group, na.rm = TRUE),
    adverbs = mean(adverbs_group, na.rm = TRUE),
    pronouns = mean(pronouns_group, na.rm = TRUE),
    interjections = mean(interjections_group, na.rm = TRUE),
    punctuation = mean(punctuation_group, na.rm = TRUE),
    function_words = mean(function_word_group, na.rm = TRUE)
  ) 
# 1970-1979
ratio_1970_1979 <- lyrics_1970_1979 %>%
  mutate( 
    nouns_group = noun / pos_total,
    proper_nouns_group = proper_noun / pos_total,
    verbs_group = (verb + AUX )/ pos_total,
    adjectives_group = adjective / pos_total,
    adverbs_group = adverb / pos_total + particle / pos_total,
    pronouns_group = pronoun / pos_total,
    interjections_group = interjection / pos_total,
    punctuation_group = punctuation / pos_total,
    function_word_group = (determiner + coordinating_conj + subordinating_conj + adposition + `NA.`) / pos_total,
  ) %>%
  summarise(
    decade = "1970_1979",
    nouns = mean(nouns_group, na.rm = TRUE),
    proper_nouns = mean(proper_nouns_group, na.rm = TRUE),
    verbs = mean(verbs_group, na.rm = TRUE),
    adjectives = mean(adjectives_group, na.rm = TRUE),
    adverbs = mean(adverbs_group, na.rm = TRUE),
    pronouns = mean(pronouns_group, na.rm = TRUE),
    interjections = mean(interjections_group, na.rm = TRUE),
    punctuation = mean(punctuation_group, na.rm = TRUE),
    function_words = mean(function_word_group, na.rm = TRUE)
  )
# 1980-1989
ratio_1980_1989 <- lyrics_1980_1989 %>%
  mutate(
    nouns_group = noun / pos_total,
    proper_nouns_group = proper_noun / pos_total,
    verbs_group = (verb + AUX )/ pos_total,
    adjectives_group = adjective / pos_total,
    adverbs_group = adverb / pos_total + particle / pos_total,
    pronouns_group = pronoun / pos_total,
    interjections_group = interjection / pos_total,
    punctuation_group = punctuation / pos_total,
    function_word_group = (determiner + coordinating_conj + subordinating_conj + adposition + `NA.`) / pos_total,
  ) %>%
  summarise(
    decade = "1980_1989",
    nouns = mean(nouns_group, na.rm = TRUE),
    proper_nouns = mean(proper_nouns_group, na.rm = TRUE),
    verbs = mean(verbs_group, na.rm = TRUE),
    adjectives = mean(adjectives_group, na.rm = TRUE),
    adverbs = mean(adverbs_group, na.rm = TRUE),
    pronouns = mean(pronouns_group, na.rm = TRUE),
    interjections = mean(interjections_group, na.rm = TRUE),
    punctuation = mean(punctuation_group, na.rm = TRUE),
    function_words = mean(function_word_group, na.rm = TRUE)
  )
# 1990-1999
ratio_1990_1999 <- lyrics_1990_1999 %>%
  mutate(
    nouns_group = noun / pos_total,
    proper_nouns_group = proper_noun / pos_total,
    verbs_group = (verb + AUX )/ pos_total,
    adjectives_group = adjective / pos_total,
    adverbs_group = adverb / pos_total + particle / pos_total,
    pronouns_group = pronoun / pos_total,
    interjections_group = interjection / pos_total,
    punctuation_group = punctuation / pos_total,
    function_word_group = (determiner + coordinating_conj + subordinating_conj + adposition + `NA.`) / pos_total,
  ) %>%
  summarise(
    decade = "1990_1999",
    nouns = mean(nouns_group, na.rm = TRUE),
    proper_nouns = mean(proper_nouns_group, na.rm = TRUE),
    verbs = mean(verbs_group, na.rm = TRUE),
    adjectives = mean(adjectives_group, na.rm = TRUE),
    adverbs = mean(adverbs_group, na.rm = TRUE),
    pronouns = mean(pronouns_group, na.rm = TRUE),
    interjections = mean(interjections_group, na.rm = TRUE),
    punctuation = mean(punctuation_group, na.rm = TRUE),
    function_words = mean(function_word_group, na.rm = TRUE)
  )
# 2000–2009
ratio_2000_2009 <- lyrics_2000_2009 %>%
  mutate(
    nouns_group = noun / pos_total,
    proper_nouns_group = proper_noun / pos_total,
    verbs_group = (verb + AUX )/ pos_total,
    adjectives_group = adjective / pos_total,
    adverbs_group = adverb / pos_total + particle / pos_total,
    pronouns_group = pronoun / pos_total,
    interjections_group = interjection / pos_total,
    punctuation_group = punctuation / pos_total,
    function_word_group = (determiner + coordinating_conj + subordinating_conj + adposition + `NA.`) / pos_total,
  ) %>%
  summarise(
    decade = "2000_2009",
    nouns = mean(nouns_group, na.rm = TRUE),
    proper_nouns = mean(proper_nouns_group, na.rm = TRUE),
    verbs = mean(verbs_group, na.rm = TRUE),
    adjectives = mean(adjectives_group, na.rm = TRUE),
    adverbs = mean(adverbs_group, na.rm = TRUE),
    pronouns = mean(pronouns_group, na.rm = TRUE),
    interjections = mean(interjections_group, na.rm = TRUE),
    punctuation = mean(punctuation_group, na.rm = TRUE),
    function_words = mean(function_word_group, na.rm = TRUE)
  )
# 2010–2019
ratio_2010_2019 <- lyrics_2010_2019 %>%
  mutate(
    nouns_group = noun / pos_total,
    proper_nouns_group = proper_noun / pos_total,
    verbs_group = (verb + AUX )/ pos_total,
    adjectives_group = adjective / pos_total,
    adverbs_group = adverb / pos_total + particle / pos_total,
    pronouns_group = pronoun / pos_total,
    interjections_group = interjection / pos_total,
    punctuation_group = punctuation / pos_total,
    function_word_group = (determiner + coordinating_conj + subordinating_conj + adposition + `NA.`) / pos_total,
  ) %>%
  summarise(
    decade = "2010_2019",
    nouns = mean(nouns_group, na.rm = TRUE),
    proper_nouns = mean(proper_nouns_group, na.rm = TRUE),
    verbs = mean(verbs_group, na.rm = TRUE),
    adjectives = mean(adjectives_group, na.rm = TRUE),
    adverbs = mean(adverbs_group, na.rm = TRUE),
    pronouns = mean(pronouns_group, na.rm = TRUE),
    interjections = mean(interjections_group, na.rm = TRUE),
    punctuation = mean(punctuation_group, na.rm = TRUE),
    function_words = mean(function_word_group, na.rm = TRUE)
  )
# 2020–2024
ratio_2020_2024 <- lyrics_2020_2024 %>%
  mutate(
    nouns_group = noun / pos_total,
    proper_nouns_group = proper_noun / pos_total,
    verbs_group = (verb + AUX )/ pos_total,
    adjectives_group = adjective / pos_total,
    adverbs_group = adverb / pos_total + particle / pos_total,
    pronouns_group = pronoun / pos_total,
    interjections_group = interjection / pos_total,
    punctuation_group = punctuation / pos_total,
    function_word_group = (determiner + coordinating_conj + subordinating_conj + adposition + `NA.`) / pos_total,
  ) %>%
  summarise(
    decade = "2020_2024",
    nouns = mean(nouns_group, na.rm = TRUE),
    proper_nouns = mean(proper_nouns_group, na.rm = TRUE),
    verbs = mean(verbs_group, na.rm = TRUE),
    adjectives = mean(adjectives_group, na.rm = TRUE),
    adverbs = mean(adverbs_group, na.rm = TRUE),
    pronouns = mean(pronouns_group, na.rm = TRUE),
    interjections = mean(interjections_group, na.rm = TRUE),
    punctuation = mean(punctuation_group, na.rm = TRUE),
    function_words = mean(function_word_group, na.rm = TRUE)
  )
full_proportion_summary <- bind_rows(
  ratio_1958_1969,
  ratio_1970_1979,
  ratio_1980_1989,
  ratio_1990_1999,
  ratio_2000_2009,
  ratio_2010_2019,
  ratio_2020_2024,
)
```

```{r, echo=FALSE}
head(full_proportion_summary[, 1:5], n = 3)
```


```{r, include=FALSE}
long_proportion_summary  <- full_proportion_summary  %>%
  pivot_longer(
    cols = -decade, 
    names_to = "pos", 
    values_to = "proportion"
    )
```


```{r, include=FALSE}
z_score_prop_summary <- long_proportion_summary %>%
  group_by(pos) %>%
  mutate(z_score = scale(proportion)) %>% # scale() standardizes the proportion within each POS category — so now we can see how each decade deviates from the average for that POS.
  ungroup()
```

## Heat Map for Z-Score Standard Normal Distribution

```{r, echo=FALSE}
ggplot(z_score_prop_summary, aes(x = decade, y = pos, fill = z_score)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(
    low = "blue", mid = "white", high = "red", midpoint = 0,
    name = "Z-Score"
  ) +
  theme_minimal(base_size = 12) +
  labs(
    title = "Z-Score Normalized POS Use by Decade",
    x = "Decade",
    y = "Part of Speech"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

Z-score is based off of a standard normal distribution

## What does the Z-score mean from the heat map?:

### Z-score > 0 :

- The proportion of that POS in this decade is above average compared to all other decades of the same POS.

- The higher the value, the more unusually high that POS usage was in that decade.

### Z-score < 0 :

- The proportion of that POS in this decade is below average compared to all other decades of the same POS.

### Z-score near 0 :

- The proportion of POS usage is very close across the average of all decades.

## Key Observations

- Between the 1980's to early 2000's, verbs usage in the Top 20 Songs of the Year were above the global average of verb usage during all other time periods. Verbs are used to describe "actions" and may be a linguistic preference to song writers from this time period. 
- Meanwhile, verb usage was significantly below the average proportion from 1958-1969. It could be because the amount of instrumental songs were popular during this decade.
- Proper Noun's i.e. names of places, personal names, etc. were more significantly found in the top songs from 1958-1969. Colloquial references to other names were also more popular in these songs.
- Across the decade the pronoun usage starting out was low in the 60's and 70's, but the average proportion of pronoun usage grew with time. This isn't a surprise because references to "He", "She", "it", "them", etc. are rather popular in romantic songs nowadays.
- Punctuations, often an indicator for slang like "Ain't", "'Til", "findin'" were more popular in the top songs from 2020-2024, and the proportion was above the standard mean.
- Interjections, i.e. "Ahoy", "Hey", "Ugh", "Gee, etc. were especially popular in the top songs from 1958-1969 often dominated by Elvis Presley, and Elton John.

## Animated Stacked Area Plot Code

```{r, include=FALSE}
full_proportion_summary <- full_proportion_summary %>%
  mutate(decade = str_replace_all(decade, "-", "_")) %>%
  mutate(decade = factor(decade, levels = c(
    "1958_1969", "1970_1979", "1980_1989", "1990_1999",
    "2000_2009", "2010_2019", "2020_2024"
  )))

```

```{r, include=FALSE}
long_proportion_summary <- long_proportion_summary %>%
  mutate(decade = factor(decade, levels = c(
    "1958_1969", "1970_1979", "1980_1989", "1990_1999",
    "2000_2009", "2010_2019", "2020_2024"
  )))

```


```{r, echo=FALSE}
ggplot(long_proportion_summary, aes(x = pos, y = proportion, fill = pos)) +
  geom_col() +
  scale_y_continuous(labels = percent_format()) +
  labs(
    title = "POS Proportions for Top Songs: {closest_state}",
    x = NULL,  # No label
    y = "Proportion of Total POS Words",
    fill = "POS Type"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    axis.ticks.x = element_blank()     # Remove x-axis ticks
  ) +
  transition_states(decade, transition_length = 3, state_length = 1) +
  ease_aes('cubic-in-out')

```

### Key Observations:
- Most of the English language structure has relatively the same proportion of parts of speech types.
- What differs are the trends that are gradually shifting the preferences of most songwriters or our semantics.
- Some likely key errors could be within the language model, either not detecting accurately the semantic meaning behind modern words or lingo used in the past or present.
- The semantic context of the lyrical sentence structure may differ.


\newpage


```{r, eval=FALSE, include=FALSE}
anim <- last_plot()
animate(anim, duration = 10, fps = 15, width = 850, height = 600, renderer = gifski_renderer("pos_animation.gif"))
```


```{r, echo=FALSE, message=FALSE}
wordcount_by_year <- bind_rows(yearly_averages)

ggplot(wordcount_by_year, aes(x = as.numeric(Year), y = avg_word_count)) +
  geom_line(color = "steelblue", linewidth = 1) +
  geom_point(color = "red", size = 1.5) +
  geom_smooth(method = "lm", color = "darkgreen", linetype = "dashed", se = TRUE) +
  labs(
    title = "Average Word Count per Year (Top 20 Billboard Songs)",
    x = "Year (1958-2024)",
    y = "Average Word Count"
  ) +
  theme_minimal()

```

## Average Amount of Words Count per Year(Top 20 Billboard Songs)

- Takes every Top Billboard Songs Per Year and measures the average word count per year
- Word counts increased among songs since 1958.


```{r, echo=FALSE, message=FALSE}
ggplot(wordcount_by_year, aes(x = as.numeric(Year), y = avg_verb + avg_auxiliary_verb)) +
  geom_line(color = "steelblue", linewidth = 1) +
  geom_point(color = "darkred", size = 1.5) +
  geom_smooth(method = "lm", color = "black", linetype = "dashed", se = TRUE) +
  labs(
    title = "Average Verbs in Lyrics per Year (Top 20 Billboard Songs)",
    x = "Year (1958-2024)",
    y = "Average Verb Count"
  ) +
  theme_minimal()

```

## Verb Count per Year
- As the same with word count, the amount of verb count per year has significantly increased since 1958.

```{r, echo=FALSE, message=FALSE}
# List of unwanted words to exclude
unwanted_words <- c("the", "and", "to", "a", "it", "it's", "an", "is", "on", "so", "at", "too", "off", 
                    "but", "that", "there", "their", "was", "by", "my", "your", "are", "am", "i'm", 
                    "this", "has", "had", "in", "of", "i'd", "i've", "oh", "these", "into", "as", "no", 
                    "ya", "only", "ah", "be", "do", "you'll", "who", "what", "when", "where")

count_words_from_df <- function(df, column_name, exclude = NULL) {
  if (!requireNamespace("stringr", quietly = TRUE)) install.packages("stringr")
  library(stringr)

  if (!column_name %in% names(df)) {
    stop("The specified column does not exist in the data frame.")
  }

  lyrics_text <- paste(df[[column_name]], collapse = " ")
  lyrics_text <- tolower(lyrics_text)
  lyrics_text <- str_replace_all(lyrics_text, "[0-9]", " ")
  lyrics_text <- str_squish(lyrics_text)

  words <- unlist(str_split(lyrics_text, "\\s+"))

  if (!is.null(exclude)) {
    # Remove punctuation from both sides before comparison
    words_clean <- str_replace_all(words, "[[:punct:]]", "")
    exclude_clean <- str_replace_all(exclude, "[[:punct:]]", "")
    words <- words[!words_clean %in% exclude_clean]
  }

  word_counts <- table(words)
  word_df <- as.data.frame(word_counts, stringsAsFactors = FALSE)
  colnames(word_df) <- c("word", "count")
  word_df <- word_df[order(-word_df$count), ]

  return(word_df)
}

# Use the function with unwanted words passed in
counting_lyrics <- count_words_from_df(lyrics_df_count, "lyrics", exclude = unwanted_words)

```

# Wordcloud: Top 200 Most Frequently Used Words in Top 20 Billboard Songs (1958-2024)

(Removed most function words e.g. "a", "an", "it", etc.)
```{r, echo=FALSE}
wordcloud(
  words = counting_lyrics$word,
  freq = counting_lyrics$count,
  min.freq = 100,                    # Only include words that appear 100+ times
  max.words = 200,                 # Limit number of words in the cloud
  random.order = FALSE,           # Plot most frequent words in center
  colors = brewer.pal(7, "Dark2") # Use a nice color palette
)

```


## Conclusions:

- Songs repeating their own titles in their lyrics have been on average increasing since 2010
- Words counts of songs have gotten longer over time
- Older songs tended to favor instrumentals or solo parts which could be why the amount of lyrical words were generally less from 1958-1979.
- Verb usage in the 1980's and 90's were proportionately in more top songs and above the standard mean across all other decades.
